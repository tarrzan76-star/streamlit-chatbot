import streamlit as st
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Optional
from difflib import SequenceMatcher
import re
from collections import Counter
import sys
import importlib
import inspect

# 1. ìºì‹œ ì´ˆê¸°í™”ëŠ” ìœ ì§€
#st.cache_data.clear()

# =========================================================================
# ğŸš€ Streamlit ì•± ë©”ì¸ ì‹¤í–‰ ë¡œì§ (ì´ˆê¸° ì„¤ì •)
# =========================================================================
st.set_page_config( 
    page_title="ì‘ì—…ê³„íšì„œ ì‘ì„±ì§€ì› ì±—ë´‡",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =========================================================================
# ğŸ“Œ ê³ ì • ìƒìˆ˜ ì •ì˜ ë° ê²½ë¡œ ì„¤ì • 
# =========================================================================
BASE_DIR = Path(__file__).parent
CSV_DIR = BASE_DIR / "input_csv" 

# ìœ„í—˜ì„± í‰ê°€ CSV íŒŒì¼ ëª©ë¡
CSV_FILENAMES = [
    "ìœ„í—˜ì„±í‰ê°€(ì„œë¶€ë³¸ë¶€).csv", "ìœ„í—˜ì„±í‰ê°€(ê²½ë¶ë³¸ë¶€).csv", "ìœ„í—˜ì„±í‰ê°€(ì„œìš¸ë³¸ë¶€).csv",
    "ìœ„í—˜ì„±í‰ê°€(ê´‘ì£¼ë³¸ë¶€).csv", "ìœ„í—˜ì„±í‰ê°€(ì „ë‚¨ë³¸ë¶€).csv", "ìœ„í—˜ì„±í‰ê°€(ëŒ€ì „ì¶©ë‚¨ë³¸ë¶€).csv",
    "ìœ„í—˜ì„±í‰ê°€(ì˜¤ì†¡ê³ ì†).csv",
]
CSV_FILES = [CSV_DIR / filename for filename in CSV_FILENAMES]

# -------------------------------------------------------------
# ğŸ’¡ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™ CSV íŒŒì¼ ëª©ë¡ (íƒ­ 2ë²ˆì˜ ê²€ìƒ‰ ëŒ€ìƒ ë°ì´í„°)
# -------------------------------------------------------------
MAINTENANCE_CSV_FILENAMES = [
    "ì •ë³´í†µì‹ ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™.csv",
    "ì‹ í˜¸ì œì–´ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™.csv",
    "ì „ì² ì „ë ¥ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™(ì†¡ë³€ì „ì„¤ë¹„).csv",
    "ì „ì² ì „ë ¥ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™(ì „ë ¥ì„¤ë¹„).csv",
    "ì „ì² ì „ë ¥ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™(ì „ì°¨ì„ ë¡œì„¤ë¹„, ê°•ì²´êµ¬ê°„).csv",
    "ì „ì² ì „ë ¥ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™(ì „ì°¨ì„ ë¡œì„¤ë¹„, ì§€ìƒêµ¬ê°„).csv",
]
MAINTENANCE_CSV_FILES = [CSV_DIR / filename for filename in MAINTENANCE_CSV_FILENAMES]
# í‘œì¤€ ìŠ¤í‚¤ë§ˆ: ì´ ìŠ¤í‚¤ë§ˆë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í†µì¼í•  ê²ƒì…ë‹ˆë‹¤.
MAINTENANCE_NEEDED_COLUMNS = ["ì„¸ì¹™ëª…", "ì„¤ë¹„ëª…", "ì ê²€í•­ëª©", "ì ê²€ì£¼ê¸°", "ì ê²€ì¢…ë¥˜"] 

# ğŸ’¡ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™ íŒŒì¼ë³„ ì»¬ëŸ¼ ë§µí•‘ ì •ì˜ (ì›ë³¸ ì»¬ëŸ¼ëª… : í‘œì¤€ ì»¬ëŸ¼ëª…)
MAINTENANCE_COLUMN_MAPS: Dict[str, Dict[str, str]] = {
    # 'ì •ë³´í†µì‹ ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™.csv'
    "ì •ë³´í†µì‹ ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™.csv": {
        "ì„¤ë¹„ëª…": "ì„¤ë¹„ëª…",
        "ì ê²€ í•­ëª©": "ì ê²€í•­ëª©", 
        "ì ê²€ì£¼ê¸°": "ì ê²€ì£¼ê¸°"
    },
    "ì‹ í˜¸ì œì–´ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™.csv": {
        "ì‹œì„¤ëª…": "ì„¤ë¹„ëª…", 
        "ì ê²€í•­ëª©": "ì ê²€í•­ëª©", 
        "ì ê²€ì£¼ê¸°": "ì ê²€ì£¼ê¸°" 
    },
    "ì „ì² ì „ë ¥ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™(ì†¡ë³€ì „ì„¤ë¹„).csv": { 
        "ì‹œì„¤ëª…": "ì„¤ë¹„ëª…", 
        "ì ê²€í•­ëª©": "ì ê²€í•­ëª©", 
        "ì ê²€ì£¼ê¸°": "ì ê²€ì£¼ê¸°", 
        "ì ê²€ì¢…ë¥˜": "ì ê²€ì¢…ë¥˜" 
    },
    "ì „ì² ì „ë ¥ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™(ì „ë ¥ì„¤ë¹„).csv": {
        "ì‹œì„¤ëª…": "ì„¤ë¹„ëª…", 
        "ì ê²€í•­ëª©": "ì ê²€í•­ëª©", 
        "ì ê²€ì£¼ê¸°": "ì ê²€ì£¼ê¸°", 
        "ì ê²€ì¢…ë¥˜": "ì ê²€ì¢…ë¥˜" 
    },
    "ì „ì² ì „ë ¥ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™(ì „ì°¨ì„ ë¡œì„¤ë¹„, ê°•ì²´êµ¬ê°„).csv": {
        "ì‹œì„¤ëª…": "ì„¤ë¹„ëª…", 
        "ì ê²€í•­ëª©": "ì ê²€í•­ëª©", 
        "ì ê²€ì£¼ê¸°": "ì ê²€ì£¼ê¸°", 
        "ì ê²€ì¢…ë¥˜": "ì ê²€ì¢…ë¥˜" 
    },
    "ì „ì² ì „ë ¥ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™(ì „ì°¨ì„ ë¡œì„¤ë¹„, ì§€ìƒêµ¬ê°„).csv": {
        "ì‹œì„¤ëª…": "ì„¤ë¹„ëª…", 
        "ì ê²€í•­ëª©": "ì ê²€í•­ëª©", 
        "ì ê²€ì£¼ê¸°": "ì ê²€ì£¼ê¸°"
    }
}

# -------------------------------------------------------------
# ğŸ’¡ ì‹œì„¤ë¬¼ ì ê²€ CSV íŒŒì¼ ëª©ë¡ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
# -------------------------------------------------------------
FACILITY_CSV_FILENAMES = [
    "ì‹œì„¤ë¬¼_ì ê²€í•­ëª©_ìŠ¹ê°•ê¸°.csv", 
    "ì‹œì„¤ë¬¼_ì ê²€í•­ëª©_ì „ì°¨ì„ .csv", 
    "ì‹œì„¤ë¬¼_ì ê²€í•­ëª©_ì‹ í˜¸ê¸°.csv",
    "ì‹œì„¤ë¬¼_ì ê²€í•­ëª©_ìŠ¤í¬ë¦°ë„ì–´.csv",
    "ì‹œì„¤ë¬¼_ì ê²€í•­ëª©_ëƒ‰ë‚œë°©ì¥ì¹˜.csv",
    "ì‹œì„¤ë¬¼_ì ê²€í•­ëª©_ê¸‰ì „ì„ .csv",
]
FACILITY_CSV_FILES = [CSV_DIR / filename for filename in FACILITY_CSV_FILENAMES]
FACILITY_NEEDED_COLUMNS = ["ì‹œì„¤ë¬¼ëª…", "ì ê²€í•­ëª©", "ì ê²€ì£¼ê¸°"] 

# ì‹œì„¤ë¬¼ ì ê²€ CSV íŒŒì¼ë³„ ì»¬ëŸ¼ ë§µí•‘ ì •ì˜
FACILITY_COLUMN_MAPS: Dict[str, Dict[str, str]] = {
    "ì‹œì„¤ë¬¼_ì ê²€í•­ëª©_ê¸‰ì „ì„ .csv": {
        "ì‹œì„¤ë¬¼ ëª…": "ì‹œì„¤ë¬¼ëª…", 
        "ì ê²€ ë‚´ìš©": "ì ê²€í•­ëª©", 
        "ì£¼ê¸°": "ì ê²€ì£¼ê¸°", 
    },
}

# =========================================================================
# ğŸ“š ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (CSV/ì„¸ì¹™ íŒŒì‹± ë° ê²€ìƒ‰) 
# =========================================================================

def _get_rule_name_from_filename(filename: str) -> str:
    name = filename.replace('.csv', '').replace('.pdf', '')
    match = re.search(r'(.+ì„¤ë¹„)\s*ìœ ì§€ë³´ìˆ˜\s*ì„¸ì¹™', name)
    if match: return match.group(1).strip() + " ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™"
    return name.strip()

def _clean_value(value: str) -> str:
    if value is None: return ""
    value = re.sub(r'[\s,]+', ' ', value).strip()
    value = re.sub(r'^[\d\s\)\.\-]+\s*|\s*[\d\s\)\.]+$', '', value).strip()
    return value
    
@st.cache_data
def load_all_safety_data(csv_file_paths, required_cols: List[str]):
    """
    ì—¬ëŸ¬ CSV íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•©í•˜ê³ , í•„ìˆ˜ ì»¬ëŸ¼ì„ ì²´í¬í•©ë‹ˆë‹¤.
    (ë¡œë”© ì‹¤íŒ¨ ë°˜ë³µ ë£¨í”„ ì œê±°ë¨)
    """
    all_dfs = []
    load_logs = []

    # [ìƒˆë¡œìš´ ì •ì˜] ì ê²€í•­ëª©ê³¼ ì ê²€ ì„¸ë¶€í•­ëª©ì„ ê²°í•©í•´ì•¼ í•˜ëŠ” íŒŒì¼ ëª©ë¡
    FILES_TO_MERGE = ["ì‹ í˜¸ì œì–´ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™.csv", "ì „ì² ì „ë ¥ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™(ì†¡ë³€ì „ì„¤ë¹„).csv"]

    for p in csv_file_paths:
        if not p.exists():
            load_logs.append((str(p), "âŒ íŒŒì¼ ì—†ìŒ"))
            continue

        df = None
        last_err = None
        
        # --- ğŸš¨ ìˆ˜ì •ëœ í•µì‹¬ ë¡œì§: ë‹¨ í•˜ë‚˜ì˜ ë¡œë”© ì‹œë„ ---
        try:
            # 1. UTF-8-SIGì™€ ì‰¼í‘œ(,)ë¡œ ìš°ì„  ì‹œë„
            df = pd.read_csv(p, encoding="utf-8-sig", sep=",", low_memory=False)
            
        except Exception:
            # 2. ì‹¤íŒ¨ ì‹œ, CP949ë¡œ ì¬ì‹œë„ (í•œê¸€ ìœˆë„ìš° ì¸ì½”ë”©)
            try:
                df = pd.read_csv(p, encoding="cp949", sep=",", low_memory=False)
            except Exception as e:
                # 3. ìµœì¢… ì‹¤íŒ¨ ê¸°ë¡
                last_err = e
                df = None
        # --- ğŸš¨ ìˆ˜ì •ëœ í•µì‹¬ ë¡œì§ ë ---
        
        # ... (íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ì²˜ë¦¬ ìƒëµ)
        if df is None:
            # ì›ì¸ ë¶„ì„ì„ ìœ„í•´ ì—ëŸ¬ ë¡œê·¸ë¥¼ ëª…í™•í•˜ê²Œ ë‚¨ê¹€
            if not load_logs or "âŒ íŒŒì¼ ì—†ìŒ" not in load_logs[-1]: 
                 load_logs.append((str(p), f"âŒ ìµœì¢… ì½ê¸° ì‹¤íŒ¨: {type(last_err).__name__} (íŒŒì¼ ì¸ì½”ë”©/í˜•ì‹ ì˜¤ë¥˜)"))
            continue

        # ì»¬ëŸ¼ ì •ë¦¬: ê³µë°±/ê°œí–‰/BOM ì œê±°
        df.columns = [c.replace("\ufeff", "").strip() for c in df.columns]

        # ë™ì˜ì–´ ë§µí•‘ (ìœ„í—˜ì„± í‰ê°€ CSVì— ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë™ì˜ì–´ë§Œ ì„ì‹œ ì ìš©)
        col_map = {
            "ì‘ì—… ëª…": "ì‘ì—…ëª…", "ì‘ì—…": "ì‘ì—…ëª…", "ìœ„í—˜ ìš”ì¸": "ìœ„í—˜ìš”ì¸", "ì•ˆì „ ì¡°ì¹˜ ë°©ë²•": "ì•ˆì „ì¡°ì¹˜ë°©ë²•",
        }
        for src, dst in col_map.items():
            if src in df.columns and dst not in df.columns:
                df[dst] = df[src]
        
        # ğŸ’¡ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™/ì‹œì„¤ë¬¼ ì ê²€ CSV íŒŒì¼ë³„ ë§ì¶¤ ì»¬ëŸ¼ ë§µí•‘ ì ìš© (rename)
        filename = p.name
        map_config = {}
        current_required_cols = list(required_cols)

        if current_required_cols == MAINTENANCE_NEEDED_COLUMNS and filename in MAINTENANCE_COLUMN_MAPS:
            map_config = MAINTENANCE_COLUMN_MAPS[filename]
        elif current_required_cols == FACILITY_NEEDED_COLUMNS and filename in FACILITY_COLUMN_MAPS:
            map_config = FACILITY_COLUMN_MAPS[filename]

        new_cols = {}
        for original, target in map_config.items():
            if original in df.columns and original != target:
                new_cols[original] = target
            
        if new_cols:
            df.rename(columns=new_cols, inplace=True)
        
        # ğŸš¨ íŠ¹ìˆ˜ ì²˜ë¦¬: 'ì ê²€í•­ëª©'ê³¼ 'ì ê²€ ì„¸ë¶€í•­ëª©' ê²°í•© ë¡œì§
        if filename in FILES_TO_MERGE:
            COL_ITEM = "ì ê²€í•­ëª©"
            COL_DETAIL = "ì ê²€ ì„¸ë¶€í•­ëª©" # ğŸš¨ ì»¬ëŸ¼ëª… í†µì¼
            
            if COL_ITEM in df.columns and COL_DETAIL in df.columns:
                
                df[COL_ITEM] = df[COL_ITEM].astype(str).str.strip()
                df[COL_DETAIL] = df[COL_DETAIL].astype(str).str.strip()
                
                mask = df[COL_DETAIL].str.len() > 0
                
                df.loc[mask, COL_ITEM] = (
                    df.loc[mask, COL_ITEM] + " (" + df.loc[mask, COL_DETAIL] + ")"
                )
                
                df.drop(columns=[COL_DETAIL], inplace=True)
                
        # íŒŒì¼ëª…ì—ì„œ ì„¸ì¹™ëª…ì„ ì¶”ì¶œí•˜ì—¬ ì»¬ëŸ¼ ì¶”ê°€ (ì„¸ì¹™ CSV ë¡œë“œ ì‹œ)
        if "ì„¸ì¹™ëª…" in current_required_cols and "ì„¸ì¹™ëª…" not in df.columns:
             df["ì„¸ì¹™ëª…"] = _get_rule_name_from_filename(p.name)

        # í•„ìˆ˜ ì—´ ì²´í¬
        final_check_cols = [
            c for c in required_cols 
            if c in df.columns or (c == "ì ê²€ ì„¸ë¶€í•­ëª©" and filename not in FILES_TO_MERGE)
        ]
        
        missing = [c for c in final_check_cols if c not in df.columns]
        
        if missing:
            load_logs.append((str(p), f"â— í•„ìˆ˜ ì—´ ëˆ„ë½: {missing} â†’ ìŠ¤í‚µ (í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)})"))
            continue
        
        # í•„ìˆ˜ ì—´ì— ëŒ€í•´ í´ë¦¬ë‹ ì ìš©
        for c in current_required_cols:
            if c not in df.columns: continue 

            df[c] = (
                df[c]
                .astype(str)
                .str.replace(r"[\r\n\t]+", " ", regex=True)
                .str.strip()
            )

        all_dfs.append(df) 
        load_logs.append((str(p), f"âœ… ë¡œë“œ ì™„ë£Œ: {len(df)} í–‰"))

    # ë¡œë“œ ìš”ì•½
    print("=== CSV LOAD LOGS ===")
    for rec in load_logs:
        print(" -", rec[0], ":", rec[1])

    if not all_dfs:
        return pd.DataFrame()

    combined = pd.concat(all_dfs, ignore_index=True).dropna(how="all")
    
    # ìµœì¢…ì ìœ¼ë¡œ í•„ìš”í•œ ì—´ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    final_cols = list(set(required_cols) & set(combined.columns))
    return combined[final_cols]

def search_csv_safety_data(df: pd.DataFrame, query: str) -> pd.DataFrame:
    """ìœ„í—˜ì„± í‰ê°€ CSVì—ì„œ ì‘ì—…ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # ... (ì´ì „ ì½”ë“œì™€ ë™ì¼, ìƒëµ)
    if df.empty or not query or "ì‘ì—…ëª…" not in df.columns: 
        return pd.DataFrame()
        
    query_lower = query.lower().strip()
    filtered_df = df[df['ì‘ì—…ëª…'].str.contains(query_lower, case=False, na=False)].copy()
    
    if not filtered_df.empty: results = filtered_df
    else:
        # ğŸš¨ 'float' object has no attribute 'lower' ì˜¤ë¥˜ ë°©ì§€ ë¡œì§ ì¶”ê°€
        unique_tasks = df['ì‘ì—…ëª…'].astype(str).unique() 
        match_ratios = {}
        for task in unique_tasks:
            if task.lower() == 'nan': continue # NaN ê°’ì€ ê±´ë„ˆëœ€
            
            ratio = SequenceMatcher(None, query_lower, task.lower()).ratio()
            if ratio > 0.55: match_ratios[task] = ratio
        
        if match_ratios:
            best_matches = sorted(match_ratios.items(), key=lambda item: item[1], reverse=True)[:5]
            best_tasks = [item[0] for item in best_matches]
            results = df[df['ì‘ì—…ëª…'].isin(best_tasks)].copy()
        else: return pd.DataFrame()
            
    if not results.empty:
        results = results[['ì‘ì—…ëª…', 'ìœ„í—˜ìš”ì¸', 'ì•ˆì „ì¡°ì¹˜ë°©ë²•']].drop_duplicates().reset_index(drop=True)
    return results

def search_maintenance_data(df: pd.DataFrame, query: str) -> pd.DataFrame:
    """ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ë¹„ëª…/ì‹œì„¤ë¬¼ëª…ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # ... (ì´ì „ ì½”ë“œì™€ ë™ì¼, ìƒëµ)
    REQUIRED_COLS = ["ì„¸ì¹™ëª…", "ì„¤ë¹„ëª…", "ì ê²€í•­ëª©", "ì ê²€ì£¼ê¸°", "ì ê²€ì¢…ë¥˜"] 
    
    if df.empty or not query or REQUIRED_COLS[1] not in df.columns:
        return pd.DataFrame()
    
    query_lower = query.lower().strip()
    
    filtered_df = df[df[REQUIRED_COLS[1]].str.contains(query_lower, case=False, na=False)].copy()
    
    if not filtered_df.empty:
        results = filtered_df[REQUIRED_COLS].drop_duplicates().reset_index(drop=True)
        return results
    else:
        # ğŸš¨ 'float' object has no attribute 'lower' ì˜¤ë¥˜ ë°©ì§€ ë¡œì§ ì¶”ê°€
        unique_facilities = df[REQUIRED_COLS[1]].astype(str).unique()
        match_ratios = {}
        for facility in unique_facilities:
            if facility.lower() == 'nan': continue # NaN ê°’ì€ ê±´ë„ˆëœ€
            
            ratio = SequenceMatcher(None, query_lower, facility.lower()).ratio()
            if ratio > 0.55: match_ratios[facility] = ratio
        
        if match_ratios:
            best_matches = sorted(match_ratios.items(), key=lambda item: item[1], reverse=True)[:5]
            best_facilities = [item[0] for item in best_matches]
            results = df[df[REQUIRED_COLS[1]].isin(best_facilities)].copy()
            results = results[REQUIRED_COLS].drop_duplicates().reset_index(drop=True)
            return results
        else:
            return pd.DataFrame()

# ğŸ¯ ë¬¸ì¥ ìš”ì•½ ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ)
def generate_maintenance_summary_sentences(df: pd.DataFrame) -> List[str]:
    """
    ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìì—°ì–´ ìš”ì•½ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.
    (ì„¤ë¹„ëª…), (ì ê²€ì£¼ê¸°), (ì ê²€í•­ëª©: ìµœëŒ€ 2ê°œ + 'ë“±'), (ì ê²€ì¢…ë¥˜)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    if df.empty:
        return []

    df['ì ê²€ì¢…ë¥˜'] = df['ì ê²€ì¢…ë¥˜'].fillna("").astype(str).str.strip()

    grouped = df.groupby(['ì„¤ë¹„ëª…', 'ì ê²€ì£¼ê¸°', 'ì ê²€ì¢…ë¥˜']).agg(
        ì ê²€í•­ëª©=('ì ê²€í•­ëª©', lambda x: list(x.unique()))
    ).reset_index()

    sentences = []
    
    for _, row in grouped.iterrows():
        facility = row['ì„¤ë¹„ëª…']
        cycle = row['ì ê²€ì£¼ê¸°']
        check_type = row['ì ê²€ì¢…ë¥˜']
        items = row['ì ê²€í•­ëª©']
        
        # 1. ì ê²€í•­ëª© ìš”ì•½ ë¡œì§: 3ê°œ ì´ìƒì´ë©´ 2ê°œ + 'ë“±' ì²˜ë¦¬
        if len(items) >= 3:
            item_summary = f"'{items[0]}', '{items[1]}' ë“±"
        elif len(items) == 2:
            item_summary = f"'{items[0]}' ë° '{items[1]}'"
        elif len(items) == 1:
            item_summary = f"'{items[0]}'"
        else:
            continue
            
        # 2. ë¬¸ì¥ êµ¬ì„±
        if check_type:
            sentence = (
                f"**{facility}**ëŠ” ì ê²€ì£¼ê¸°(**{cycle}**)ì— ë”°ë¼ {item_summary}ì˜ ì ê²€í•­ëª©ì„ **{check_type}** ì ê²€í•´ì•¼ í•©ë‹ˆë‹¤."
            )
        else:
             sentence = (
                f"**{facility}**ëŠ” ì ê²€ì£¼ê¸°(**{cycle}**)ì— ë”°ë¼ {item_summary}ì˜ ì ê²€í•­ëª©ì„ ì ê²€í•´ì•¼ í•©ë‹ˆë‹¤."
            )
        
        sentences.append(sentence)
        
    return sentences

# =========================================================================
# ğŸš€ Streamlit ì•± ë©”ì¸ ì‹¤í–‰ ë¡œì§ (ë°ì´í„° ë¡œë“œ ë° UI)
# =========================================================================

# 1. ë°ì´í„° ë¡œë“œ (ìºì‹± ì ìš©)
data_df = load_all_safety_data(csv_file_paths=CSV_FILES, required_cols=["ì‘ì—…ëª…", "ìœ„í—˜ìš”ì¸", "ì•ˆì „ì¡°ì¹˜ë°©ë²•"]) 

maintenance_df = load_all_safety_data(
    csv_file_paths=MAINTENANCE_CSV_FILES, 
    required_cols=MAINTENANCE_NEEDED_COLUMNS
)

# 2. ì•ˆì „ì¥ì¹˜
if not isinstance(data_df, pd.DataFrame): data_df = pd.DataFrame()
if not isinstance(maintenance_df, pd.DataFrame): maintenance_df = pd.DataFrame()


# --- ì‚¬ì´ë“œë°” ë° ë¡œë”© ìƒíƒœ í™•ì¸ ---
st.title("ğŸ›¡ï¸ ì‘ì—…ê³„íšì„œ ì‘ì„±ì§€ì› ì±—ë´‡")
st.caption("ìœ„í—˜ì„± í‰ê°€ ë°ì´í„°, ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™ ë“±ì„ ê¸°ë°˜ìœ¼ë¡œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

# ë°ì´í„° ë¡œë”© ìƒíƒœ í‘œì‹œ
with st.sidebar:
    st.header("ğŸ“Š ë°ì´í„° ë¡œë”© ìƒíƒœ") 
    
    if not data_df.empty:
        st.success(f"ğŸ’¾ **1. ìœ„í—˜ì„±í‰ê°€** í•­ëª© ìˆ˜: {len(data_df):,}ê°œ")
    else:
        st.error("âŒ **1. ìœ„í—˜ì„±í‰ê°€** ë¡œë“œ ì‹¤íŒ¨")

    if not maintenance_df.empty:
        st.success(f"ğŸ’¾ **2. ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™** í•­ëª© ìˆ˜: {len(maintenance_df):,}ê°œ")
    else:
        st.error("âŒ **2. ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™** ë¡œë“œ ì‹¤íŒ¨") 
    
    st.markdown("---")
    

# --- ë©”ì¸ íƒ­ ì •ì˜ ---
tab1, tab2 = st.tabs([
    "ğŸ” ì‘ì—…ëª… ê²€ìƒ‰ (ìœ„í—˜ì„± í‰ê°€)", 
    "âš™ï¸ ì‹œì„¤ë¬¼ ê²€ìƒ‰ (ì ê²€í•­ëª© ë° ì£¼ê¸°)", 
]) 

# =========================================================
# tab1: ì‘ì—…ëª… ê²€ìƒ‰ (ìœ„í—˜ì„± í‰ê°€ CSV ê²€ìƒ‰) - ë³€ê²½ ì—†ìŒ
# =========================================================
with tab1:
    
    st.subheader("1. ì‘ì—…ëª… ê¸°ë°˜ ìœ„í—˜ì„± í‰ê°€ ê²€ìƒ‰ (CSV)")
    
    csv_query = st.text_input(
        "ê²€ìƒ‰í•  **ì‘ì—…ëª…**ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ìŠ¹ê°•ê¸° ìœ ì§€ë³´ìˆ˜, ì „ì°¨ì„  ì‘ì—…):", 
        key="csv_input"
    )
    
    if st.button("ìœ„í—˜ì„±í‰ê°€ ê²€ìƒ‰", key="csv_button") and csv_query:
        if data_df.empty:
              st.warning("âš ï¸ ìœ„í—˜ì„± í‰ê°€ CSV ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ê²€ìƒ‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì‚¬ì´ë“œë°” í™•ì¸)")
        else:
            with st.spinner(f"'{csv_query}'ì— ëŒ€í•œ ìœ„í—˜ì„± í‰ê°€ ê²€ìƒ‰ ì¤‘..."):
                csv_results = search_csv_safety_data(data_df, csv_query)
            
            if csv_results.empty:
                st.warning(f"ğŸš¨ **'{csv_query}'** (ë˜ëŠ” ìœ ì‚¬í•œ ì‘ì—…ëª…)ì— ëŒ€í•œ ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.success(f"âœ… **'{csv_query}'**ì™€ ê°€ì¥ ìœ ì‚¬í•œ {len(csv_results):,}ê°œì˜ ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼ì…ë‹ˆë‹¤.")
                
                st.dataframe(csv_results, use_container_width=True)
                
                st.markdown("### ğŸ“ ì£¼ìš” ìœ„í—˜ìš”ì¸ ë° ì•ˆì „ ì¡°ì¹˜ ìš”ì•½")
                top_risks = Counter(csv_results['ìœ„í—˜ìš”ì¸']).most_common(5)
                top_actions = Counter(csv_results['ì•ˆì „ì¡°ì¹˜ë°©ë²•']).most_common(5)
                
                st.markdown("#### ìœ„í—˜ìš”ì¸ (Top 5)")
                for risk, count in top_risks: st.markdown(f"- **{risk}** (ë°˜ë³µ {count}íšŒ)")
                
                st.markdown("#### ì•ˆì „ì¡°ì¹˜ë°©ë²• (Top 5)")
                for action, count in top_actions: st.markdown(f"- **{action}** (ë°˜ë³µ {count}íšŒ)")


# =========================================================
# tab2: ì‹œì„¤ë¬¼ ê²€ìƒ‰ (ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™ ê¸°ë°˜) - 'ì ê²€í•­ëª© ë° ì£¼ê¸° ìš”ì•½' ì‚­ì œ
# =========================================================
with tab2:
    st.subheader("1. ì„¤ë¹„ëª… ê¸°ë°˜ ì ê²€ í•­ëª© ë° ì£¼ê¸° ê²€ìƒ‰ (ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™)")
    
    maintenance_query = st.text_input(
        "ê²€ìƒ‰í•  **ì„¤ë¹„ëª…/ì‹œì„¤ë¬¼ëª…**ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë³€ì••ê¸°, ì‹ í˜¸ê¸°ì¥ì¹˜):", 
        key="maintenance_input"
    )
    
    if st.button("ì‹œì„¤ë¬¼/ì„¸ì¹™ ê²€ìƒ‰", key="maintenance_button") and maintenance_query:
        if maintenance_df.empty: 
              st.warning("âš ï¸ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ê²€ìƒ‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì‚¬ì´ë“œë°” í™•ì¸)")
        else:
            with st.spinner(f"'{maintenance_query}'ì— ëŒ€í•œ ì ê²€ ì •ë³´ ê²€ìƒ‰ ì¤‘..."):
                maintenance_results = search_maintenance_data(maintenance_df, maintenance_query) 
            
            if maintenance_results.empty:
                st.warning(f"ğŸš¨ **'{maintenance_query}'**ì— ëŒ€í•œ ì ê²€ ì •ë³´ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (ìœ ì‚¬ë„ ê²€ìƒ‰ í¬í•¨)")
            else:
                st.success(f"âœ… **'{maintenance_query}'**ì™€ ê´€ë ¨ëœ {len(maintenance_results):,}ê°œì˜ ì ê²€ ì •ë³´ ê²°ê³¼ì…ë‹ˆë‹¤.")

                # --- 1. ìƒì„¸ ì¡°íšŒ ê²°ê³¼ (DataFrame) ---
                st.subheader("ìƒì„¸ ì¡°íšŒ ê²°ê³¼")
                display_cols = ["ì„¸ì¹™ëª…", "ì„¤ë¹„ëª…", "ì ê²€ì£¼ê¸°", "ì ê²€ì¢…ë¥˜", "ì ê²€í•­ëª©"]
                display_cols = [c for c in display_cols if c in maintenance_results.columns]
                
                st.dataframe(maintenance_results[display_cols], use_container_width=True)
                
                # --- 2. ë¬¸ì¥ ìš”ì•½ ê²°ê³¼ (ì‹ ê·œ ìš”ì²­) ---
                summary_sentences = generate_maintenance_summary_sentences(maintenance_results)
                
                st.markdown("---")
                st.subheader("ğŸ’¬ ë¬¸ì¥ ìš”ì•½ ê²°ê³¼")
                
                if summary_sentences:
                    for sentence in summary_sentences:
                        st.markdown(f"- {sentence}")
                else:

                    st.info("ë¬¸ì¥ ìš”ì•½ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
