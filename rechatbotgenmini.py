import streamlit as st
import pandas as pd
from pathlib import Path
from typing import List, Dict
from difflib import SequenceMatcher
import re
from collections import Counter
import warnings

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ UI ë° ê²½ë¡œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="ì‘ì—…ê³„íšì„œ ì‘ì„±ì§€ì› + ë¬¸ì„œ QA (Gemini)",
    layout="wide",
    initial_sidebar_state="expanded",
)

BASE_DIR = Path(__file__).parent
CSV_DIR = BASE_DIR / "input_csv"
CSV_DIR.mkdir(exist_ok=True)

RISK_CSV_GLOBS = ["ìœ„í—˜ì„±í‰ê°€(*).csv"]
MAINT_CSV_GLOBS = ["*ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™*.csv"]

MAINTENANCE_NEEDED_COLUMNS = ["ì„¸ì¹™ëª…", "ì„¤ë¹„ëª…", "ì ê²€í•­ëª©", "ì ê²€ì£¼ê¸°", "ì ê²€ì¢…ë¥˜"]

MAINTENANCE_COLUMN_MAPS: Dict[str, Dict[str, str]] = {
    "ì •ë³´í†µì‹ ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™.csv": {"ì„¤ë¹„ëª…": "ì„¤ë¹„ëª…", "ì ê²€ í•­ëª©": "ì ê²€í•­ëª©", "ì ê²€ì£¼ê¸°": "ì ê²€ì£¼ê¸°"},
    "ì‹ í˜¸ì œì–´ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™.csv": {
        "ì‹œì„¤ëª…": "ì„¤ë¹„ëª…", "ì ê²€í•­ëª©": "ì ê²€í•­ëª©", "ì ê²€ ì„¸ë¶€í•­ëª©": "ì ê²€ ì„¸ë¶€í•­ëª©", "ì ê²€ì£¼ê¸°": "ì ê²€ì£¼ê¸°"},
    "ì „ì² ì „ë ¥ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™(ì†¡ë³€ì „ì„¤ë¹„).csv": {
        "ì‹œì„¤ëª…": "ì„¤ë¹„ëª…", "ì ê²€í•­ëª©": "ì ê²€í•­ëª©", "ì ê²€ ì„¸ë¶€í•­ëª©": "ì ê²€ ì„¸ë¶€í•­ëª©",
        "ì ê²€ì£¼ê¸°": "ì ê²€ì£¼ê¸°", "ì ê²€ì¢…ë¥˜": "ì ê²€ì¢…ë¥˜"},
    "ì „ì² ì „ë ¥ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™(ì „ë ¥ì„¤ë¹„).csv": {
        "ì‹œì„¤ëª…": "ì„¤ë¹„ëª…", "ì ê²€í•­ëª©": "ì ê²€í•­ëª©", "ì ê²€ì£¼ê¸°": "ì ê²€ì£¼ê¸°", "ì ê²€ì¢…ë¥˜": "ì ê²€ì¢…ë¥˜"},
    "ì „ì² ì „ë ¥ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™(ì „ì°¨ì„ ë¡œì„¤ë¹„, ê°•ì²´êµ¬ê°„).csv": {
        "ì‹œì„¤ëª…": "ì„¤ë¹„ëª…", "ì ê²€í•­ëª©": "ì ê²€í•­ëª©", "ì ê²€ì£¼ê¸°": "ì ê²€ì£¼ê¸°", "ì ê²€ì¢…ë¥˜": "ì ê²€ì¢…ë¥˜"},
    "ì „ì² ì „ë ¥ì„¤ë¹„ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™(ì „ì°¨ì„ ë¡œì„¤ë¹„, ì§€ìƒêµ¬ê°„).csv": {
        "ì‹œì„¤ëª…": "ì„¤ë¹„ëª…", "ì ê²€í•­ëª©": "ì ê²€í•­ëª©", "ì ê²€ì£¼ê¸°": "ì ê²€ì£¼ê¸°"},
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CSV ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_rule_name_from_filename(filename: str) -> str:
    name = filename.replace(".csv", "").replace(".pdf", "")
    m = re.search(r"(.+ì„¤ë¹„)\s*ìœ ì§€ë³´ìˆ˜\s*ì„¸ì¹™", name)
    return (m.group(1).strip() + " ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™") if m else name.strip()

@st.cache_data(show_spinner=False)
def _read_csv_any(path: Path) -> pd.DataFrame:
    try:
        return pd.read_csv(path, encoding="utf-8-sig", low_memory=False)
    except Exception:
        return pd.read_csv(path, encoding="cp949", low_memory=False)

@st.cache_data(show_spinner=False)
def load_risk_data() -> pd.DataFrame:
    all_dfs = []
    for pattern in RISK_CSV_GLOBS:
        for p in CSV_DIR.glob(pattern):
            try:
                df = _read_csv_any(p)
                df.columns = [c.replace("\ufeff", "").strip() for c in df.columns]
                if "ì‘ì—…ëª…" not in df.columns:
                    if "ì‘ì—… ëª…" in df.columns: df["ì‘ì—…ëª…"] = df["ì‘ì—… ëª…"]
                    elif "ì‘ì—…" in df.columns: df["ì‘ì—…ëª…"] = df["ì‘ì—…"]
                if "ìœ„í—˜ìš”ì¸" not in df.columns and "ìœ„í—˜ ìš”ì¸" in df.columns:
                    df["ìœ„í—˜ìš”ì¸"] = df["ìœ„í—˜ ìš”ì¸"]
                if "ì•ˆì „ì¡°ì¹˜ë°©ë²•" not in df.columns and "ì•ˆì „ ì¡°ì¹˜ ë°©ë²•" in df.columns:
                    df["ì•ˆì „ì¡°ì¹˜ë°©ë²•"] = df["ì•ˆì „ ì¡°ì¹˜ ë°©ë²•"]

                need = [c for c in ["ì‘ì—…ëª…", "ìœ„í—˜ìš”ì¸", "ì•ˆì „ì¡°ì¹˜ë°©ë²•"] if c in df.columns]
                if need:
                    df = df[need].copy()
                    for c in need:
                        df[c] = df[c].astype(str).str.replace(r"[\r\n\t]+", " ", regex=True).str.strip()
                    all_dfs.append(df)
            except Exception as e:
                st.warning(f"CSV ë¡œë“œ ì‹¤íŒ¨: {p.name} â†’ {type(e).__name__}")
    return pd.concat(all_dfs, ignore_index=True).dropna(how="all") if all_dfs else pd.DataFrame(columns=["ì‘ì—…ëª…","ìœ„í—˜ìš”ì¸","ì•ˆì „ì¡°ì¹˜ë°©ë²•"])

@st.cache_data(show_spinner=False)
def load_maint_data() -> pd.DataFrame:
    all_dfs = []
    for pattern in MAINT_CSV_GLOBS:
        for p in CSV_DIR.glob(pattern):
            try:
                df = _read_csv_any(p)
                df.columns = [c.replace("\ufeff", "").strip() for c in df.columns]
                if p.name in MAINTENANCE_COLUMN_MAPS:
                    df = df.rename(columns=MAINTENANCE_COLUMN_MAPS[p.name])
                if "ì„¸ì¹™ëª…" not in df.columns:
                    df["ì„¸ì¹™ëª…"] = _get_rule_name_from_filename(p.name)
                if "ì ê²€í•­ëª©" in df.columns and "ì ê²€ ì„¸ë¶€í•­ëª©" in df.columns:
                    df["ì ê²€í•­ëª©"] = df["ì ê²€í•­ëª©"].astype(str).str.strip()
                    df["ì ê²€ ì„¸ë¶€í•­ëª©"] = df["ì ê²€ ì„¸ë¶€í•­ëª©"].astype(str).str.strip()
                    mask = df["ì ê²€ ì„¸ë¶€í•­ëª©"].str.len() > 0
                    df.loc[mask, "ì ê²€í•­ëª©"] = df.loc[mask, "ì ê²€í•­ëª©"] + " (" + df.loc[mask, "ì ê²€ ì„¸ë¶€í•­ëª©"] + ")"
                    df.drop(columns=["ì ê²€ ì„¸ë¶€í•­ëª©"], inplace=True)
                for col in MAINTENANCE_NEEDED_COLUMNS:
                    if col not in df.columns: df[col] = ""
                for c in MAINTENANCE_NEEDED_COLUMNS:
                    df[c] = df[c].astype(str).str.replace(r"[\r\n\t]+", " ", regex=True).str.strip()
                all_dfs.append(df[MAINTENANCE_NEEDED_COLUMNS])
            except Exception as e:
                st.warning(f"ì„¸ì¹™ CSV ë¡œë“œ ì‹¤íŒ¨: {p.name} â†’ {type(e).__name__}")
    return pd.concat(all_dfs, ignore_index=True).dropna(how="all") if all_dfs else pd.DataFrame(columns=MAINTENANCE_NEEDED_COLUMNS)

def search_risk(df: pd.DataFrame, q: str) -> pd.DataFrame:
    if df.empty or not q or "ì‘ì—…ëª…" not in df.columns:
        return pd.DataFrame(columns=["ì‘ì—…ëª…","ìœ„í—˜ìš”ì¸","ì•ˆì „ì¡°ì¹˜ë°©ë²•"])
    ql = q.lower().strip()
    exact = df[df["ì‘ì—…ëª…"].str.contains(ql, case=False, na=False)].copy()
    if not exact.empty:
        return exact[["ì‘ì—…ëª…","ìœ„í—˜ìš”ì¸","ì•ˆì „ì¡°ì¹˜ë°©ë²•"]].drop_duplicates().reset_index(drop=True)
    scores = {}
    for t in df["ì‘ì—…ëª…"].astype(str).unique():
        if t.lower() == "nan": continue
        r = SequenceMatcher(None, ql, t.lower()).ratio()
        if r > 0.55: scores[t] = r
    if not scores: return pd.DataFrame(columns=["ì‘ì—…ëª…","ìœ„í—˜ìš”ì¸","ì•ˆì „ì¡°ì¹˜ë°©ë²•"])
    best = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:5]
    return df[df["ì‘ì—…ëª…"].isin([b[0] for b in best])][["ì‘ì—…ëª…","ìœ„í—˜ìš”ì¸","ì•ˆì „ì¡°ì¹˜ë°©ë²•"]].drop_duplicates().reset_index(drop=True)

def search_maint(df: pd.DataFrame, q: str) -> pd.DataFrame:
    cols = MAINTENANCE_NEEDED_COLUMNS
    if df.empty or not q or cols[1] not in df.columns:
        return pd.DataFrame(columns=cols)
    ql = q.lower().strip()
    exact = df[df[cols[1]].str.contains(ql, case=False, na=False)].copy()
    if not exact.empty: return exact[cols].drop_duplicates().reset_index(drop=True)
    scores = {}
    for name in df[cols[1]].astype(str).unique():
        if name.lower() == "nan": continue
        r = SequenceMatcher(None, ql, name.lower()).ratio()
        if r > 0.55: scores[name] = r
    if not scores: return pd.DataFrame(columns=cols)
    best = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:5]
    return df[df[cols[1]].isin([b[0] for b in best])][cols].drop_duplicates().reset_index(drop=True)

def summarize_maint(df: pd.DataFrame) -> List[str]:
    if df.empty: return []
    df = df.copy()
    if "ì ê²€ì¢…ë¥˜" in df.columns:
        df["ì ê²€ì¢…ë¥˜"] = df["ì ê²€ì¢…ë¥˜"].fillna("").astype(str).str.strip()
    group_cols = [c for c in ["ì„¤ë¹„ëª…","ì ê²€ì£¼ê¸°","ì ê²€ì¢…ë¥˜"] if c in df.columns]
    grouped = df.groupby(group_cols).agg(ì ê²€í•­ëª©=("ì ê²€í•­ëª©", lambda x: list(pd.Series(x).dropna().astype(str).unique()))).reset_index()
    sents = []
    for _, r in grouped.iterrows():
        items = r["ì ê²€í•­ëª©"]
        if   len(items) >= 3: item_txt = f"'{items[0]}', '{items[1]}' ë“±"
        elif len(items) == 2: item_txt = f"'{items[0]}' ë° '{items[1]}'"
        elif len(items) == 1: item_txt = f"'{items[0]}'"
        else: continue
        if r.get("ì ê²€ì¢…ë¥˜", ""):
            s = f"**{r['ì„¤ë¹„ëª…']}**ëŠ” ì ê²€ì£¼ê¸°(**{r['ì ê²€ì£¼ê¸°']}**)ì— ë”°ë¼ {item_txt}ì˜ ì ê²€í•­ëª©ì„ **{r['ì ê²€ì¢…ë¥˜']}** ì ê²€í•´ì•¼ í•©ë‹ˆë‹¤."
        else:
            s = f"**{r['ì„¤ë¹„ëª…']}**ëŠ” ì ê²€ì£¼ê¸°(**{r['ì ê²€ì£¼ê¸°']}**)ì— ë”°ë¼ {item_txt}ì˜ ì ê²€í•­ëª©ì„ ì ê²€í•´ì•¼ í•©ë‹ˆë‹¤."
        sents.append(s)
    return sents

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¬¸ì„œ QA (ì§€ì—° ë¡œë”©: ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ importëŠ” í•¨ìˆ˜ ì•ˆì—ì„œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_embeddings():
    from langchain_community.embeddings import HuggingFaceEmbeddings
    return HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )

def get_text(uploaded_docs):
    from loguru import logger
    from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
    docs = []
    for doc in uploaded_docs:
        file_name = doc.name
        with open(file_name, "wb") as f:
            f.write(doc.getvalue())
            logger.info(f"Uploaded {file_name}")
        if file_name.lower().endswith(".pdf"):
            loader = PyPDFLoader(file_name)
        elif file_name.lower().endswith(".docx"):
            loader = Docx2txtLoader(file_name)
        else:
            continue
        docs.extend(loader.load_and_split())
    return docs

def tiktoken_len(text: str) -> int:
    try:
        import tiktoken
        tok = tiktoken.get_encoding("cl100k_base")
        return len(tok.encode(text))
    except Exception:
        return len(text)

def split_chunks(documents):
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=900, chunk_overlap=100, length_function=tiktoken_len
    )
    return splitter.split_documents(documents)

def chunks_to_vectordb(chunks):
    from langchain_community.vectorstores import FAISS
    emb = build_embeddings()
    return FAISS.from_documents(chunks, emb)

def get_conversation_chain(vstore, gemini_api_key: str):
    from langchain.chains import ConversationalRetrievalChain
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain.memory import ConversationBufferMemory

    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-pro",
        google_api_key=gemini_api_key,
        temperature=0
    )
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type="stuff",
        retriever=vstore.as_retriever(search_type="mmr", search_kwargs={"k": 4}),
        memory=ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer"),
        get_chat_history=lambda h: h,
        return_source_documents=True,
        verbose=False,
    )
    return chain

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ›¡ï¸ ì‘ì—…ê³„íšì„œ ì‘ì„±ì§€ì› ì±—ë´‡ + ğŸ“š ë¬¸ì„œ QA (Gemini)")
st.caption("CSV ê¸°ë°˜ ìœ„í—˜ì„±/ì„¸ì¹™ ê²€ìƒ‰ê³¼ ì—…ë¡œë“œ ë¬¸ì„œ QAë¥¼ í•œ ê³³ì—ì„œ!")

with st.sidebar:
    st.subheader("ğŸ”§ ê³µí†µ ì„¤ì •")
    gemini_api_key = st.secrets.get("GEMINI_API_KEY", "")
    if not gemini_api_key:
        st.info("Secretsì— GEMINI_API_KEYë¥¼ ë“±ë¡í•˜ì„¸ìš”. (â‹¯ â†’ Edit secrets)")
    st.markdown("---")
    st.subheader("ğŸ“š ë¬¸ì„œ ì—…ë¡œë“œ (QA)")
    uploaded_files = st.file_uploader(
        "PDF / DOCX íŒŒì¼ ì—…ë¡œë“œ",
        type=["pdf", "docx"],
        accept_multiple_files=True
    )
    process_docs = st.button("ë¬¸ì„œ ì„ë² ë”© ìƒì„±")

if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None
if "messages" not in st.session_state:
    st.session_state.messages = [{"role":"assistant","content":"ì•ˆë…•í•˜ì„¸ìš”! ë¬¸ì„œ QA ë˜ëŠ” CSV ê²€ìƒ‰ íƒ­ì—ì„œ ì‹œì‘í•´ ë³´ì„¸ìš”."}]

qa_tab, risk_tab, maint_tab = st.tabs(["ğŸ“– ë¬¸ì„œ QA", "ğŸ” ìœ„í—˜ì„± í‰ê°€ (CSV)", "âš™ï¸ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™ (CSV)"])

with qa_tab:
    st.subheader("ë¬¸ì„œ ê¸°ë°˜ Q&A")
    st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•œ ë’¤ **ë¬¸ì„œ ì„ë² ë”© ìƒì„±**ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    if process_docs:
        if not gemini_api_key:
            st.warning("Gemini API Keyë¥¼ Secretsì— ì¶”ê°€í•´ ì£¼ì„¸ìš”."); st.stop()
        if not uploaded_files:
            st.warning("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”."); st.stop()
        with st.spinner("ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘... (ìµœì´ˆ ì‹¤í–‰ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
            docs = get_text(uploaded_files)
            chunks = split_chunks(docs)
            vdb = chunks_to_vectordb(chunks)
            st.session_state.qa_chain = get_conversation_chain(vdb, gemini_api_key)
        st.success("ë¬¸ì„œ ì„ë² ë”© ë° ì²´ì¸ ì¤€ë¹„ ì™„ë£Œ!")

    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    user_q = st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
    if user_q:
        if st.session_state.qa_chain is None:
            st.warning("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì„ë² ë”©ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.")
        else:
            st.session_state.messages.append({"role":"user","content":user_q})
            with st.chat_message("user"):
                st.markdown(user_q)
            with st.chat_message("assistant"):
                with st.spinner("Thinking..."):
                    result = st.session_state.qa_chain({"question": user_q})
                    answer = result.get("answer","")
                    srcs = result.get("source_documents",[])
                    st.markdown(answer)
                    if srcs:
                        with st.expander("ì°¸ê³  ë¬¸ì„œ ë³´ê¸°"):
                            for i, d in enumerate(srcs[:5], start=1):
                                src = d.metadata.get("source","unknown")
                                page = d.metadata.get("page", None)
                                meta = f"{src}" + (f" (p.{page+1})" if isinstance(page, int) else "")
                                st.markdown(f"**{i}.** {meta}", help=d.page_content[:1000])
            st.session_state.messages.append({"role":"assistant","content":answer})

with risk_tab:
    st.subheader("ì‘ì—…ëª… ê¸°ë°˜ ìœ„í—˜ì„± í‰ê°€ ê²€ìƒ‰")
    risk_df = load_risk_data()
    if risk_df.empty:
        st.error("input_csv í´ë”ì—ì„œ ìœ„í—˜ì„±í‰ê°€ CSVë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì¶”ê°€í•˜ì„¸ìš”.")
    else:
        st.success(f"ë¡œë“œëœ ìœ„í—˜ì„±í‰ê°€ í•­ëª©: {len(risk_df):,}ê°œ")
    q = st.text_input("ê²€ìƒ‰í•  ì‘ì—…ëª… (ì˜ˆ: ìŠ¹ê°•ê¸° ìœ ì§€ë³´ìˆ˜, ì „ì°¨ì„  ì‘ì—… ë“±)")
    if st.button("ìœ„í—˜ì„±í‰ê°€ ê²€ìƒ‰") and q:
        with st.spinner(f"'{q}' ê²€ìƒ‰ ì¤‘..."):
            res = search_risk(risk_df, q)
        if res.empty:
            st.warning(f"'{q}' ê´€ë ¨ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤ (ìœ ì‚¬ë„ ê²€ìƒ‰ í¬í•¨).")
        else:
            st.success(f"'{q}'ì™€ ê´€ë ¨ëœ {len(res):,}ê°œ ê²°ê³¼")
            st.dataframe(res, use_container_width=True)
            st.markdown("### ğŸ“ ì£¼ìš” ìœ„í—˜ìš”ì¸ / ì•ˆì „ì¡°ì¹˜ Top 5")
            top_risks = Counter(res["ìœ„í—˜ìš”ì¸"]).most_common(5)
            top_actions = Counter(res["ì•ˆì „ì¡°ì¹˜ë°©ë²•"]).most_common(5)
            c1, c2 = st.columns(2)
            with c1:
                st.markdown("#### ìœ„í—˜ìš”ì¸")
                for t, c in top_risks: st.markdown(f"- **{t}** (ë°˜ë³µ {c}íšŒ)")
            with c2:
                st.markdown("#### ì•ˆì „ì¡°ì¹˜ë°©ë²•")
                for t, c in top_actions: st.markdown(f"- **{t}** (ë°˜ë³µ {c}íšŒ)")

with maint_tab:
    st.subheader("ì„¤ë¹„ëª… ê¸°ë°˜ ì ê²€ í•­ëª© ë° ì£¼ê¸° (ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™)")
    maint_df = load_maint_data()
    if maint_df.empty:
        st.error("input_csv í´ë”ì—ì„œ ìœ ì§€ë³´ìˆ˜ ì„¸ì¹™ CSVë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì¶”ê°€í•˜ì„¸ìš”.")
    else:
        st.success(f"ë¡œë“œëœ ì„¸ì¹™ í•­ëª©: {len(maint_df):,}ê°œ")
    q2 = st.text_input("ì„¤ë¹„ëª…/ì‹œì„¤ë¬¼ëª… (ì˜ˆ: ë³€ì••ê¸°, ì‹ í˜¸ê¸°ì¥ì¹˜ ë“±)", key="maint_q")
    if st.button("ì„¸ì¹™ ê²€ìƒ‰", key="maint_btn") and q2:
        with st.spinner(f"'{q2}' ê²€ìƒ‰ ì¤‘..."):
            res = search_maint(maint_df, q2)
        if res.empty:
            st.warning(f"'{q2}' ê´€ë ¨ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤ (ìœ ì‚¬ë„ ê²€ìƒ‰ í¬í•¨).")
        else:
            st.success(f"'{q2}' ê´€ë ¨ {len(res):,}ê°œ ê²°ê³¼")
            show_cols = [c for c in ["ì„¸ì¹™ëª…","ì„¤ë¹„ëª…","ì ê²€ì£¼ê¸°","ì ê²€ì¢…ë¥˜","ì ê²€í•­ëª©"] if c in res.columns]
            st.dataframe(res[show_cols], use_container_width=True)
            st.markdown("---")
            st.subheader("ğŸ’¬ ë¬¸ì¥ ìš”ì•½ ê²°ê³¼")
            for s in summarize_maint(res):
                st.markdown(f"- {s}")
